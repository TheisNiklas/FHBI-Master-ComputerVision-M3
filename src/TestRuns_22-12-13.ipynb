{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import PIL\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "from utils.modelLoader import ModelLoader\n",
    "import numpy as np\n",
    "from utils.utilities import buildRunName"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes for every age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7824 validated image filenames belonging to 99 classes.\n",
      "Found 1956 validated image filenames belonging to 99 classes.\n",
      "Epoch 1/50\n",
      "245/245 [==============================] - ETA: 0s - loss: 3.6924 - accuracy: 0.1465"
     ]
    }
   ],
   "source": [
    "images = pd.read_json(\"../data_meta/age_crop/meta_full_str.json\")\n",
    "images['Age'] = images['Age'].apply(lambda x: str(x))\n",
    "\n",
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_images = train_generator.flow_from_dataframe(\n",
    "    dataframe=images,\n",
    "    x_col='Filepath',\n",
    "    y_col='Age',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_images = train_generator.flow_from_dataframe(\n",
    "    dataframe=images,\n",
    "    x_col='Filepath',\n",
    "    y_col='Age',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "model = ModelLoader().loadMobileNetV1Age(train_images, False, False, train_images.class_indices.__len__())\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[\n",
    "        \"accuracy\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "log_dir = \"../logs/fit/\" + buildRunName(\"MobileNet_Class_Crop_2\", 50, 32)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "history = model.fit(\n",
    "    train_images,\n",
    "    validation_data=val_images,\n",
    "    epochs=50,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")\n",
    "\n",
    "model.save(\"../models/\" + buildRunName(\"MobileNet_Class_Crop_2\", 50, 32))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Training to get age value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7824 validated image filenames.\n",
      "Found 1956 validated image filenames.\n",
      "Epoch 1/50\n",
      "245/245 [==============================] - 44s 161ms/step - loss: 144.3967 - accuracy: 0.0271 - val_loss: 1333.9303 - val_accuracy: 0.0440\n",
      "Epoch 2/50\n",
      "245/245 [==============================] - 38s 155ms/step - loss: 65.5305 - accuracy: 0.0603 - val_loss: 5487.1689 - val_accuracy: 0.1125\n",
      "Epoch 3/50\n",
      "245/245 [==============================] - 39s 158ms/step - loss: 51.8126 - accuracy: 0.0661 - val_loss: 2239.2549 - val_accuracy: 0.1125\n",
      "Epoch 4/50\n",
      "245/245 [==============================] - 39s 160ms/step - loss: 35.8886 - accuracy: 0.0718 - val_loss: 397.6191 - val_accuracy: 0.1125\n",
      "Epoch 5/50\n",
      "245/245 [==============================] - 39s 159ms/step - loss: 27.4254 - accuracy: 0.0729 - val_loss: 1114.1238 - val_accuracy: 0.1125\n",
      "Epoch 6/50\n",
      "245/245 [==============================] - 39s 158ms/step - loss: 24.8917 - accuracy: 0.0708 - val_loss: 276.9601 - val_accuracy: 0.1115\n",
      "Epoch 7/50\n",
      "245/245 [==============================] - 39s 157ms/step - loss: 20.8535 - accuracy: 0.0817 - val_loss: 2387.6790 - val_accuracy: 0.1125\n",
      "Epoch 8/50\n",
      "245/245 [==============================] - 39s 158ms/step - loss: 16.6552 - accuracy: 0.0785 - val_loss: 214.9846 - val_accuracy: 0.1115\n",
      "Epoch 9/50\n",
      "245/245 [==============================] - 40s 161ms/step - loss: 13.9099 - accuracy: 0.0846 - val_loss: 518.3632 - val_accuracy: 0.1125\n",
      "Epoch 10/50\n",
      "245/245 [==============================] - 38s 157ms/step - loss: 11.3726 - accuracy: 0.0840 - val_loss: 2596.5522 - val_accuracy: 0.1125\n",
      "Epoch 11/50\n",
      "245/245 [==============================] - 39s 158ms/step - loss: 10.1100 - accuracy: 0.0821 - val_loss: 89.6384 - val_accuracy: 0.1048\n",
      "Epoch 12/50\n",
      "245/245 [==============================] - 39s 158ms/step - loss: 9.1095 - accuracy: 0.0840 - val_loss: 201.7183 - val_accuracy: 0.1125\n",
      "Epoch 13/50\n",
      "245/245 [==============================] - 39s 158ms/step - loss: 8.6081 - accuracy: 0.0877 - val_loss: 1689.7131 - val_accuracy: 0.1125\n",
      "Epoch 14/50\n",
      "245/245 [==============================] - 39s 158ms/step - loss: 8.0286 - accuracy: 0.0851 - val_loss: 108.9181 - val_accuracy: 0.1022\n",
      "Epoch 15/50\n",
      "245/245 [==============================] - 39s 157ms/step - loss: 9.2638 - accuracy: 0.0886 - val_loss: 4981.9844 - val_accuracy: 0.1125\n",
      "Epoch 16/50\n",
      "245/245 [==============================] - 38s 156ms/step - loss: 10.6154 - accuracy: 0.0887 - val_loss: 492.4185 - val_accuracy: 0.1063\n",
      "Epoch 17/50\n",
      "245/245 [==============================] - 39s 157ms/step - loss: 10.5121 - accuracy: 0.0901 - val_loss: 2002.3616 - val_accuracy: 0.1125\n",
      "Epoch 18/50\n",
      "245/245 [==============================] - 40s 162ms/step - loss: 10.5070 - accuracy: 0.0920 - val_loss: 3940.0342 - val_accuracy: 0.1125\n",
      "Epoch 19/50\n",
      "245/245 [==============================] - 39s 159ms/step - loss: 11.2596 - accuracy: 0.0858 - val_loss: 223.3338 - val_accuracy: 0.1125\n",
      "Epoch 20/50\n",
      "245/245 [==============================] - 38s 157ms/step - loss: 9.9724 - accuracy: 0.0881 - val_loss: 1117.5651 - val_accuracy: 0.1125\n",
      "Epoch 21/50\n",
      "245/245 [==============================] - 39s 158ms/step - loss: 8.2970 - accuracy: 0.0874 - val_loss: 86.6023 - val_accuracy: 0.1125\n",
      "Epoch 22/50\n",
      "245/245 [==============================] - 39s 158ms/step - loss: 6.2396 - accuracy: 0.0925 - val_loss: 891.6740 - val_accuracy: 0.1125\n",
      "Epoch 23/50\n",
      "245/245 [==============================] - 40s 162ms/step - loss: 5.7443 - accuracy: 0.0971 - val_loss: 219.1428 - val_accuracy: 0.1115\n",
      "Epoch 24/50\n",
      "245/245 [==============================] - 39s 157ms/step - loss: 5.1717 - accuracy: 0.0975 - val_loss: 1121.2271 - val_accuracy: 0.1125\n",
      "Epoch 25/50\n",
      "245/245 [==============================] - 39s 157ms/step - loss: 5.0115 - accuracy: 0.0979 - val_loss: 69.4727 - val_accuracy: 0.1115\n",
      "Epoch 26/50\n",
      "245/245 [==============================] - 38s 157ms/step - loss: 4.8824 - accuracy: 0.0971 - val_loss: 291.2383 - val_accuracy: 0.1125\n",
      "Epoch 27/50\n",
      "245/245 [==============================] - 39s 159ms/step - loss: 5.1655 - accuracy: 0.0989 - val_loss: 701.0685 - val_accuracy: 0.1125\n",
      "Epoch 28/50\n",
      "245/245 [==============================] - 38s 157ms/step - loss: 5.9958 - accuracy: 0.1010 - val_loss: 68.5678 - val_accuracy: 0.1115\n",
      "Epoch 29/50\n",
      "245/245 [==============================] - 39s 158ms/step - loss: 18.3919 - accuracy: 0.0960 - val_loss: 1606.9175 - val_accuracy: 0.1125\n",
      "Epoch 30/50\n",
      "245/245 [==============================] - 39s 158ms/step - loss: 21.5281 - accuracy: 0.0851 - val_loss: 2935.2776 - val_accuracy: 0.1125\n",
      "Epoch 31/50\n",
      "245/245 [==============================] - 38s 157ms/step - loss: 9.1598 - accuracy: 0.0904 - val_loss: 1991.3433 - val_accuracy: 0.1125\n",
      "Epoch 32/50\n",
      "245/245 [==============================] - 40s 162ms/step - loss: 5.6736 - accuracy: 0.0955 - val_loss: 79.8244 - val_accuracy: 0.1125\n",
      "Epoch 33/50\n",
      "245/245 [==============================] - 38s 156ms/step - loss: 4.4956 - accuracy: 0.0992 - val_loss: 67.8057 - val_accuracy: 0.0905\n",
      "Epoch 34/50\n",
      "245/245 [==============================] - 39s 158ms/step - loss: 3.7662 - accuracy: 0.0993 - val_loss: 65.5859 - val_accuracy: 0.1109\n",
      "Epoch 35/50\n",
      "245/245 [==============================] - 39s 158ms/step - loss: 3.5623 - accuracy: 0.1001 - val_loss: 175.0806 - val_accuracy: 0.1120\n",
      "Epoch 36/50\n",
      "245/245 [==============================] - 39s 158ms/step - loss: 3.3670 - accuracy: 0.1007 - val_loss: 178.0981 - val_accuracy: 0.1125\n",
      "Epoch 37/50\n",
      "245/245 [==============================] - 39s 158ms/step - loss: 3.3945 - accuracy: 0.1031 - val_loss: 73.4608 - val_accuracy: 0.1099\n",
      "Epoch 38/50\n",
      "245/245 [==============================] - 39s 157ms/step - loss: 3.3652 - accuracy: 0.1026 - val_loss: 368.9221 - val_accuracy: 0.1125\n",
      "Epoch 39/50\n",
      "245/245 [==============================] - 39s 158ms/step - loss: 3.6315 - accuracy: 0.1037 - val_loss: 81.6529 - val_accuracy: 0.1125\n",
      "Epoch 40/50\n",
      "245/245 [==============================] - 39s 157ms/step - loss: 4.0327 - accuracy: 0.1043 - val_loss: 64.8522 - val_accuracy: 0.0961\n",
      "Epoch 41/50\n",
      "245/245 [==============================] - 39s 158ms/step - loss: 3.8995 - accuracy: 0.1040 - val_loss: 579.1904 - val_accuracy: 0.1125\n",
      "Epoch 42/50\n",
      "245/245 [==============================] - 38s 157ms/step - loss: 4.0788 - accuracy: 0.1033 - val_loss: 242.3719 - val_accuracy: 0.1109\n",
      "Epoch 43/50\n",
      "245/245 [==============================] - 39s 157ms/step - loss: 4.0689 - accuracy: 0.1051 - val_loss: 117.6958 - val_accuracy: 0.1125\n",
      "Epoch 44/50\n",
      "245/245 [==============================] - 39s 159ms/step - loss: 4.1704 - accuracy: 0.1043 - val_loss: 679.2402 - val_accuracy: 0.1125\n",
      "Epoch 45/50\n",
      "245/245 [==============================] - 39s 158ms/step - loss: 4.3204 - accuracy: 0.1047 - val_loss: 402.6503 - val_accuracy: 0.1125\n",
      "Epoch 46/50\n",
      "245/245 [==============================] - 40s 163ms/step - loss: 4.9060 - accuracy: 0.1005 - val_loss: 218.5012 - val_accuracy: 0.1125\n",
      "Epoch 47/50\n",
      "245/245 [==============================] - 38s 156ms/step - loss: 13.7365 - accuracy: 0.0974 - val_loss: 330.6301 - val_accuracy: 0.1125\n",
      "Epoch 48/50\n",
      "245/245 [==============================] - 39s 159ms/step - loss: 12.3133 - accuracy: 0.0971 - val_loss: 272.1241 - val_accuracy: 0.1125\n",
      "Epoch 49/50\n",
      "245/245 [==============================] - 39s 158ms/step - loss: 5.4001 - accuracy: 0.1016 - val_loss: 75.1388 - val_accuracy: 0.1115\n",
      "Epoch 50/50\n",
      "245/245 [==============================] - 39s 158ms/step - loss: 3.4019 - accuracy: 0.1035 - val_loss: 69.2281 - val_accuracy: 0.1120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 27). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/MobileNet_Regression_Crop_epochs-50_batch-32\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/MobileNet_Regression_Crop_epochs-50_batch-32\\assets\n"
     ]
    }
   ],
   "source": [
    "images = pd.read_json(\"../data_meta/age_crop/meta_full.json\")\n",
    "\n",
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_images = train_generator.flow_from_dataframe(\n",
    "    dataframe=images,\n",
    "    x_col='Filepath',\n",
    "    y_col='Age',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='raw',\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_images = train_generator.flow_from_dataframe(\n",
    "    dataframe=images,\n",
    "    x_col='Filepath',\n",
    "    y_col='Age',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='raw',\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "model = ModelLoader().loadMobileNetV1Age(train_images, False, False, 1)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=[\n",
    "        \"accuracy\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "log_dir = \"../logs/fit/\" + buildRunName(\"MobileNet_Regression_Crop\", 50, 32)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "history = model.fit(\n",
    "    train_images,\n",
    "    validation_data=val_images,\n",
    "    epochs=50,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")\n",
    "\n",
    "model.save(\"../models/\" + buildRunName(\"MobileNet_Regression_Crop\", 50, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[91.447266]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = tf.io.read_file(\"../data/age_crop/96_1_0_20170110182019881.jpg.chip.jpg\")\n",
    "image = tf.image.decode_image(image, channels=3)\n",
    "image = np.expand_dims(image.numpy(), axis=0)\n",
    "image = tf.image.resize(image, (224,224))\n",
    "model.predict(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes for ages in steps of 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7824 validated image filenames belonging to 23 classes.\n",
      "Found 1956 validated image filenames belonging to 23 classes.\n",
      "Epoch 1/50\n",
      "245/245 [==============================] - 46s 169ms/step - loss: 1.9100 - accuracy: 0.3767 - val_loss: 7.1425 - val_accuracy: 0.0813\n",
      "Epoch 2/50\n",
      "245/245 [==============================] - 40s 162ms/step - loss: 1.5039 - accuracy: 0.4521 - val_loss: 2.7931 - val_accuracy: 0.1994\n",
      "Epoch 3/50\n",
      "245/245 [==============================] - 40s 163ms/step - loss: 1.3462 - accuracy: 0.4967 - val_loss: 4.7398 - val_accuracy: 0.0608\n",
      "Epoch 4/50\n",
      "245/245 [==============================] - 40s 164ms/step - loss: 1.2349 - accuracy: 0.5337 - val_loss: 2.5960 - val_accuracy: 0.1973\n",
      "Epoch 5/50\n",
      "245/245 [==============================] - 40s 163ms/step - loss: 1.0912 - accuracy: 0.5907 - val_loss: 2.3080 - val_accuracy: 0.3241\n",
      "Epoch 6/50\n",
      "245/245 [==============================] - 40s 163ms/step - loss: 0.9977 - accuracy: 0.6214 - val_loss: 4.3378 - val_accuracy: 0.0302\n",
      "Epoch 7/50\n",
      "245/245 [==============================] - 40s 162ms/step - loss: 0.8629 - accuracy: 0.6797 - val_loss: 3.3399 - val_accuracy: 0.1063\n",
      "Epoch 8/50\n",
      "245/245 [==============================] - 39s 159ms/step - loss: 0.7680 - accuracy: 0.7209 - val_loss: 5.9089 - val_accuracy: 0.1452\n",
      "Epoch 9/50\n",
      "245/245 [==============================] - 38s 155ms/step - loss: 0.5849 - accuracy: 0.7901 - val_loss: 5.3435 - val_accuracy: 0.2086\n",
      "Epoch 10/50\n",
      "245/245 [==============================] - 39s 160ms/step - loss: 0.5003 - accuracy: 0.8191 - val_loss: 6.0187 - val_accuracy: 0.2444\n",
      "Epoch 11/50\n",
      "245/245 [==============================] - 38s 155ms/step - loss: 0.4096 - accuracy: 0.8562 - val_loss: 3.7993 - val_accuracy: 0.3154\n",
      "Epoch 12/50\n",
      "245/245 [==============================] - 38s 157ms/step - loss: 0.3263 - accuracy: 0.8887 - val_loss: 5.1772 - val_accuracy: 0.3282\n",
      "Epoch 13/50\n",
      "245/245 [==============================] - 38s 155ms/step - loss: 0.2749 - accuracy: 0.9052 - val_loss: 3.9722 - val_accuracy: 0.2510\n",
      "Epoch 14/50\n",
      "245/245 [==============================] - 38s 157ms/step - loss: 0.2139 - accuracy: 0.9324 - val_loss: 3.6919 - val_accuracy: 0.2035\n",
      "Epoch 15/50\n",
      "245/245 [==============================] - 38s 156ms/step - loss: 0.2000 - accuracy: 0.9347 - val_loss: 4.3414 - val_accuracy: 0.1933\n",
      "Epoch 16/50\n",
      "245/245 [==============================] - 38s 156ms/step - loss: 0.2075 - accuracy: 0.9329 - val_loss: 5.5800 - val_accuracy: 0.1917\n",
      "Epoch 17/50\n",
      "245/245 [==============================] - 38s 156ms/step - loss: 0.1666 - accuracy: 0.9476 - val_loss: 5.1980 - val_accuracy: 0.1493\n",
      "Epoch 18/50\n",
      "245/245 [==============================] - 38s 155ms/step - loss: 0.1429 - accuracy: 0.9564 - val_loss: 6.0779 - val_accuracy: 0.1994\n",
      "Epoch 19/50\n",
      "245/245 [==============================] - 38s 156ms/step - loss: 0.1628 - accuracy: 0.9491 - val_loss: 6.5551 - val_accuracy: 0.2843\n",
      "Epoch 20/50\n",
      "245/245 [==============================] - 38s 157ms/step - loss: 0.1595 - accuracy: 0.9523 - val_loss: 4.0521 - val_accuracy: 0.2975\n",
      "Epoch 21/50\n",
      "245/245 [==============================] - 38s 157ms/step - loss: 0.1495 - accuracy: 0.9535 - val_loss: 4.9437 - val_accuracy: 0.0501\n",
      "Epoch 22/50\n",
      "245/245 [==============================] - 38s 156ms/step - loss: 0.1501 - accuracy: 0.9530 - val_loss: 8.2995 - val_accuracy: 0.1345\n",
      "Epoch 23/50\n",
      "245/245 [==============================] - 38s 156ms/step - loss: 0.1284 - accuracy: 0.9614 - val_loss: 5.8263 - val_accuracy: 0.2766\n",
      "Epoch 24/50\n",
      "245/245 [==============================] - 40s 162ms/step - loss: 0.1366 - accuracy: 0.9577 - val_loss: 5.5850 - val_accuracy: 0.1283\n",
      "Epoch 25/50\n",
      "245/245 [==============================] - 38s 155ms/step - loss: 0.1404 - accuracy: 0.9530 - val_loss: 4.7461 - val_accuracy: 0.1984\n",
      "Epoch 26/50\n",
      "245/245 [==============================] - 38s 156ms/step - loss: 0.1329 - accuracy: 0.9581 - val_loss: 5.6282 - val_accuracy: 0.3226\n",
      "Epoch 27/50\n",
      "245/245 [==============================] - 38s 156ms/step - loss: 0.0986 - accuracy: 0.9689 - val_loss: 3.8727 - val_accuracy: 0.1979\n",
      "Epoch 28/50\n",
      "245/245 [==============================] - 38s 157ms/step - loss: 0.0916 - accuracy: 0.9700 - val_loss: 7.2436 - val_accuracy: 0.2198\n",
      "Epoch 29/50\n",
      "245/245 [==============================] - 39s 161ms/step - loss: 0.1142 - accuracy: 0.9642 - val_loss: 5.9628 - val_accuracy: 0.2505\n",
      "Epoch 30/50\n",
      "245/245 [==============================] - 38s 156ms/step - loss: 0.1536 - accuracy: 0.9499 - val_loss: 5.4402 - val_accuracy: 0.2050\n",
      "Epoch 31/50\n",
      "245/245 [==============================] - 39s 158ms/step - loss: 0.1189 - accuracy: 0.9632 - val_loss: 3.0814 - val_accuracy: 0.3742\n",
      "Epoch 32/50\n",
      "245/245 [==============================] - 38s 156ms/step - loss: 0.0933 - accuracy: 0.9684 - val_loss: 3.5023 - val_accuracy: 0.3354\n",
      "Epoch 33/50\n",
      "245/245 [==============================] - 39s 158ms/step - loss: 0.0979 - accuracy: 0.9692 - val_loss: 4.0945 - val_accuracy: 0.2035\n",
      "Epoch 34/50\n",
      "245/245 [==============================] - 39s 158ms/step - loss: 0.0783 - accuracy: 0.9742 - val_loss: 5.6199 - val_accuracy: 0.3763\n",
      "Epoch 35/50\n",
      "245/245 [==============================] - 38s 156ms/step - loss: 0.0934 - accuracy: 0.9698 - val_loss: 5.6936 - val_accuracy: 0.1897\n",
      "Epoch 36/50\n",
      "245/245 [==============================] - 38s 156ms/step - loss: 0.0801 - accuracy: 0.9741 - val_loss: 6.1626 - val_accuracy: 0.1968\n",
      "Epoch 37/50\n",
      "245/245 [==============================] - 38s 156ms/step - loss: 0.0956 - accuracy: 0.9684 - val_loss: 5.4292 - val_accuracy: 0.2664\n",
      "Epoch 38/50\n",
      "245/245 [==============================] - 39s 161ms/step - loss: 0.0825 - accuracy: 0.9751 - val_loss: 6.1986 - val_accuracy: 0.0547\n",
      "Epoch 39/50\n",
      "245/245 [==============================] - 38s 156ms/step - loss: 0.1047 - accuracy: 0.9649 - val_loss: 11.2723 - val_accuracy: 0.0322\n",
      "Epoch 40/50\n",
      "245/245 [==============================] - 38s 157ms/step - loss: 0.1024 - accuracy: 0.9651 - val_loss: 6.0195 - val_accuracy: 0.2817\n",
      "Epoch 41/50\n",
      "245/245 [==============================] - 38s 156ms/step - loss: 0.0742 - accuracy: 0.9756 - val_loss: 4.8028 - val_accuracy: 0.2086\n",
      "Epoch 42/50\n",
      "245/245 [==============================] - 38s 156ms/step - loss: 0.0737 - accuracy: 0.9734 - val_loss: 5.5348 - val_accuracy: 0.1656\n",
      "Epoch 43/50\n",
      "245/245 [==============================] - 39s 160ms/step - loss: 0.0857 - accuracy: 0.9716 - val_loss: 6.3203 - val_accuracy: 0.0833\n",
      "Epoch 44/50\n",
      "245/245 [==============================] - 38s 156ms/step - loss: 0.0969 - accuracy: 0.9650 - val_loss: 7.8610 - val_accuracy: 0.2863\n",
      "Epoch 45/50\n",
      "245/245 [==============================] - 38s 156ms/step - loss: 0.0870 - accuracy: 0.9705 - val_loss: 6.5926 - val_accuracy: 0.0782\n",
      "Epoch 46/50\n",
      "245/245 [==============================] - 39s 158ms/step - loss: 0.0772 - accuracy: 0.9741 - val_loss: 6.9017 - val_accuracy: 0.2009\n",
      "Epoch 47/50\n",
      "245/245 [==============================] - 39s 158ms/step - loss: 0.0541 - accuracy: 0.9816 - val_loss: 7.6259 - val_accuracy: 0.2444\n",
      "Epoch 48/50\n",
      "245/245 [==============================] - 38s 155ms/step - loss: 0.0753 - accuracy: 0.9728 - val_loss: 5.2149 - val_accuracy: 0.3328\n",
      "Epoch 49/50\n",
      "245/245 [==============================] - 38s 155ms/step - loss: 0.0812 - accuracy: 0.9720 - val_loss: 4.1123 - val_accuracy: 0.2863\n",
      "Epoch 50/50\n",
      "245/245 [==============================] - 38s 155ms/step - loss: 0.0818 - accuracy: 0.9719 - val_loss: 3.5942 - val_accuracy: 0.2633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 27). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/MobileNet_ClassGroups_Crop_epochs-50_batch-32\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/MobileNet_ClassGroups_Crop_epochs-50_batch-32\\assets\n"
     ]
    }
   ],
   "source": [
    "images = pd.read_json(\"../data_meta/age_crop/meta_full_str_grouped.json\")\n",
    "images['Age'] = images['Age'].apply(lambda x: str(x))\n",
    "\n",
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_images = train_generator.flow_from_dataframe(\n",
    "    dataframe=images,\n",
    "    x_col='Filepath',\n",
    "    y_col='Age',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_images = train_generator.flow_from_dataframe(\n",
    "    dataframe=images,\n",
    "    x_col='Filepath',\n",
    "    y_col='Age',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "model = ModelLoader().loadMobileNetV1Age(train_images, False, False, train_images.class_indices.__len__())\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[\n",
    "        \"accuracy\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "log_dir = \"../logs/fit/\" + buildRunName(\"MobileNet_ClassGroups_Crop\", 50, 32)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "history = model.fit(\n",
    "    train_images,\n",
    "    validation_data=val_images,\n",
    "    epochs=50,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")\n",
    "\n",
    "model.save(\"../models/\" + buildRunName(\"MobileNet_ClassGroups_Crop\", 50, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7824 validated image filenames belonging to 23 classes.\n",
      "Found 1956 validated image filenames belonging to 23 classes.\n",
      "Epoch 1/50\n",
      "245/245 [==============================] - 154s 589ms/step - loss: 2.0229 - accuracy: 0.3662 - val_loss: 6.8034 - val_accuracy: 0.0813\n",
      "Epoch 2/50\n",
      "245/245 [==============================] - 37s 151ms/step - loss: 1.5627 - accuracy: 0.4378 - val_loss: 7.1707 - val_accuracy: 0.0608\n",
      "Epoch 3/50\n",
      "245/245 [==============================] - 38s 155ms/step - loss: 1.4097 - accuracy: 0.4755 - val_loss: 3.2429 - val_accuracy: 0.1391\n",
      "Epoch 4/50\n",
      "245/245 [==============================] - 37s 150ms/step - loss: 1.2958 - accuracy: 0.5056 - val_loss: 3.4881 - val_accuracy: 0.1043\n",
      "Epoch 5/50\n",
      "245/245 [==============================] - 37s 150ms/step - loss: 1.1879 - accuracy: 0.5435 - val_loss: 3.0197 - val_accuracy: 0.2403\n",
      "Epoch 6/50\n",
      "245/245 [==============================] - 37s 151ms/step - loss: 1.1093 - accuracy: 0.5704 - val_loss: 2.8088 - val_accuracy: 0.3221\n",
      "Epoch 7/50\n",
      "245/245 [==============================] - 37s 150ms/step - loss: 1.0070 - accuracy: 0.6090 - val_loss: 3.0811 - val_accuracy: 0.1186\n",
      "Epoch 8/50\n",
      "245/245 [==============================] - 37s 150ms/step - loss: 0.9434 - accuracy: 0.6407 - val_loss: 4.1375 - val_accuracy: 0.0240\n",
      "Epoch 9/50\n",
      "245/245 [==============================] - 37s 151ms/step - loss: 0.8354 - accuracy: 0.6787 - val_loss: 5.4529 - val_accuracy: 0.1161\n",
      "Epoch 10/50\n",
      "245/245 [==============================] - 37s 150ms/step - loss: 0.7233 - accuracy: 0.7301 - val_loss: 8.1716 - val_accuracy: 0.0179\n",
      "Epoch 11/50\n",
      "245/245 [==============================] - 37s 151ms/step - loss: 0.6552 - accuracy: 0.7538 - val_loss: 2.6340 - val_accuracy: 0.3476\n",
      "Epoch 12/50\n",
      "245/245 [==============================] - 37s 150ms/step - loss: 0.5274 - accuracy: 0.8115 - val_loss: 5.8485 - val_accuracy: 0.0107\n",
      "Epoch 13/50\n",
      "245/245 [==============================] - 37s 151ms/step - loss: 0.4359 - accuracy: 0.8432 - val_loss: 4.8816 - val_accuracy: 0.0322\n",
      "Epoch 14/50\n",
      "245/245 [==============================] - 36s 148ms/step - loss: 0.3469 - accuracy: 0.8802 - val_loss: 5.9815 - val_accuracy: 0.2658\n",
      "Epoch 15/50\n",
      "245/245 [==============================] - 37s 149ms/step - loss: 0.2957 - accuracy: 0.8981 - val_loss: 5.2599 - val_accuracy: 0.1544\n",
      "Epoch 16/50\n",
      "245/245 [==============================] - 37s 149ms/step - loss: 0.2932 - accuracy: 0.9017 - val_loss: 3.1585 - val_accuracy: 0.3124\n",
      "Epoch 17/50\n",
      "245/245 [==============================] - 37s 150ms/step - loss: 0.2582 - accuracy: 0.9130 - val_loss: 6.6295 - val_accuracy: 0.0435\n",
      "Epoch 18/50\n",
      "245/245 [==============================] - 37s 152ms/step - loss: 0.2275 - accuracy: 0.9260 - val_loss: 4.6668 - val_accuracy: 0.0685\n",
      "Epoch 19/50\n",
      "245/245 [==============================] - 37s 151ms/step - loss: 0.1887 - accuracy: 0.9392 - val_loss: 8.2723 - val_accuracy: 0.0547\n",
      "Epoch 20/50\n",
      "245/245 [==============================] - 37s 149ms/step - loss: 0.1706 - accuracy: 0.9473 - val_loss: 7.0201 - val_accuracy: 0.0041\n",
      "Epoch 21/50\n",
      "245/245 [==============================] - 36s 148ms/step - loss: 0.1909 - accuracy: 0.9398 - val_loss: 9.6040 - val_accuracy: 0.0194\n",
      "Epoch 22/50\n",
      "245/245 [==============================] - 37s 149ms/step - loss: 0.1613 - accuracy: 0.9482 - val_loss: 3.5857 - val_accuracy: 0.1682\n",
      "Epoch 23/50\n",
      "245/245 [==============================] - 37s 151ms/step - loss: 0.1517 - accuracy: 0.9532 - val_loss: 7.4504 - val_accuracy: 0.3149\n",
      "Epoch 24/50\n",
      "245/245 [==============================] - 36s 149ms/step - loss: 0.1385 - accuracy: 0.9572 - val_loss: 4.9599 - val_accuracy: 0.1442\n",
      "Epoch 25/50\n",
      "245/245 [==============================] - 37s 149ms/step - loss: 0.1378 - accuracy: 0.9551 - val_loss: 11.4730 - val_accuracy: 0.0337\n",
      "Epoch 26/50\n",
      "245/245 [==============================] - 37s 152ms/step - loss: 0.1570 - accuracy: 0.9490 - val_loss: 6.0810 - val_accuracy: 0.0158\n",
      "Epoch 27/50\n",
      "245/245 [==============================] - 37s 149ms/step - loss: 0.1667 - accuracy: 0.9477 - val_loss: 11.6623 - val_accuracy: 0.0056\n",
      "Epoch 28/50\n",
      "245/245 [==============================] - 37s 151ms/step - loss: 0.1183 - accuracy: 0.9614 - val_loss: 5.9912 - val_accuracy: 0.0435\n",
      "Epoch 29/50\n",
      "245/245 [==============================] - 37s 149ms/step - loss: 0.1095 - accuracy: 0.9631 - val_loss: 13.0712 - val_accuracy: 0.0153\n",
      "Epoch 30/50\n",
      "245/245 [==============================] - 37s 149ms/step - loss: 0.1510 - accuracy: 0.9496 - val_loss: 9.2098 - val_accuracy: 0.0598\n",
      "Epoch 31/50\n",
      "245/245 [==============================] - 37s 150ms/step - loss: 0.1519 - accuracy: 0.9526 - val_loss: 5.6472 - val_accuracy: 0.0051\n",
      "Epoch 32/50\n",
      "245/245 [==============================] - 37s 149ms/step - loss: 0.1180 - accuracy: 0.9619 - val_loss: 5.4998 - val_accuracy: 0.0475\n",
      "Epoch 33/50\n",
      "245/245 [==============================] - 37s 151ms/step - loss: 0.0892 - accuracy: 0.9734 - val_loss: 6.0434 - val_accuracy: 0.0435\n",
      "Epoch 34/50\n",
      "245/245 [==============================] - 37s 149ms/step - loss: 0.1023 - accuracy: 0.9642 - val_loss: 9.3004 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/50\n",
      "245/245 [==============================] - 36s 149ms/step - loss: 0.1194 - accuracy: 0.9601 - val_loss: 5.2277 - val_accuracy: 0.0102\n",
      "Epoch 36/50\n",
      "245/245 [==============================] - 37s 149ms/step - loss: 0.1011 - accuracy: 0.9668 - val_loss: 4.0666 - val_accuracy: 0.3221\n",
      "Epoch 37/50\n",
      "245/245 [==============================] - 37s 149ms/step - loss: 0.1046 - accuracy: 0.9645 - val_loss: 4.5536 - val_accuracy: 0.2081\n",
      "Epoch 38/50\n",
      "245/245 [==============================] - 37s 151ms/step - loss: 0.1079 - accuracy: 0.9659 - val_loss: 4.5304 - val_accuracy: 0.3502\n",
      "Epoch 39/50\n",
      "245/245 [==============================] - 37s 149ms/step - loss: 0.1020 - accuracy: 0.9663 - val_loss: 5.5942 - val_accuracy: 0.2439\n",
      "Epoch 40/50\n",
      "245/245 [==============================] - 37s 149ms/step - loss: 0.1226 - accuracy: 0.9590 - val_loss: 8.8086 - val_accuracy: 0.0706\n",
      "Epoch 41/50\n",
      "245/245 [==============================] - 37s 149ms/step - loss: 0.1013 - accuracy: 0.9660 - val_loss: 4.3251 - val_accuracy: 0.3093\n",
      "Epoch 42/50\n",
      "245/245 [==============================] - 37s 150ms/step - loss: 0.1111 - accuracy: 0.9631 - val_loss: 15.7131 - val_accuracy: 0.0557\n",
      "Epoch 43/50\n",
      "245/245 [==============================] - 38s 154ms/step - loss: 0.0862 - accuracy: 0.9712 - val_loss: 4.9922 - val_accuracy: 0.1789\n",
      "Epoch 44/50\n",
      "245/245 [==============================] - 37s 150ms/step - loss: 0.0815 - accuracy: 0.9733 - val_loss: 6.1362 - val_accuracy: 0.1043\n",
      "Epoch 45/50\n",
      "245/245 [==============================] - 37s 149ms/step - loss: 0.0725 - accuracy: 0.9753 - val_loss: 5.7514 - val_accuracy: 0.2132\n",
      "Epoch 46/50\n",
      "245/245 [==============================] - 37s 149ms/step - loss: 0.0881 - accuracy: 0.9703 - val_loss: 9.6975 - val_accuracy: 0.0046\n",
      "Epoch 47/50\n",
      "245/245 [==============================] - 37s 149ms/step - loss: 0.1145 - accuracy: 0.9626 - val_loss: 7.1330 - val_accuracy: 0.2868\n",
      "Epoch 48/50\n",
      "245/245 [==============================] - 37s 151ms/step - loss: 0.0688 - accuracy: 0.9762 - val_loss: 7.9316 - val_accuracy: 0.2745\n",
      "Epoch 49/50\n",
      "245/245 [==============================] - 37s 149ms/step - loss: 0.0554 - accuracy: 0.9811 - val_loss: 4.9615 - val_accuracy: 0.2735\n",
      "Epoch 50/50\n",
      "245/245 [==============================] - 37s 150ms/step - loss: 0.0891 - accuracy: 0.9707 - val_loss: 7.1550 - val_accuracy: 0.2311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 27). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/MobileNet_ClassGroups_Crop_epochs-50_batch-32\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/MobileNet_ClassGroups_Crop_epochs-50_batch-32\\assets\n"
     ]
    }
   ],
   "source": [
    "images = pd.read_json(\"../data_meta/age_crop/meta_full_str_grouped.json\")\n",
    "images['Age'] = images['Age'].apply(lambda x: str(x))\n",
    "\n",
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_images = train_generator.flow_from_dataframe(\n",
    "    dataframe=images,\n",
    "    x_col='Filepath',\n",
    "    y_col='Age',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_images = train_generator.flow_from_dataframe(\n",
    "    dataframe=images,\n",
    "    x_col='Filepath',\n",
    "    y_col='Age',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "model = ModelLoader().loadMobileNetV1Age(train_images, False, False, train_images.class_indices.__len__())\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[\n",
    "        \"accuracy\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "log_dir = \"../logs/fit/\" + buildRunName(\"MobileNet_ClassGroups_Crop\", 50, 32)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "history = model.fit(\n",
    "    train_images,\n",
    "    validation_data=val_images,\n",
    "    epochs=50,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")\n",
    "\n",
    "model.save(\"../models/\" + buildRunName(\"MobileNet_ClassGroups_Crop\", 50, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 598ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.4125100e-06, 1.1227400e-08, 1.6664218e-07, 9.0333669e-09,\n",
       "        1.6541505e-09, 8.3594944e-15, 5.9712266e-09, 1.1642211e-11,\n",
       "        6.3201750e-14, 4.2122063e-11, 1.2187860e-11, 4.9518213e-13,\n",
       "        3.2612607e-11, 7.7769291e-10, 2.8571100e-11, 7.9557712e-09,\n",
       "        1.2495052e-08, 1.3635757e-07, 3.0350086e-06, 4.8706561e-04,\n",
       "        9.9950433e-01, 5.7582692e-08, 2.7445412e-06]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = tf.io.read_file(\"../data/age/54_0_3_20170119210218868.jpg\")\n",
    "image = tf.image.decode_image(image, channels=3)\n",
    "image = np.expand_dims(image.numpy(), axis=0)\n",
    "image = tf.image.resize(image, (224,224))\n",
    "model.predict(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5 (tags/v3.9.5:0a7dcbd, May  3 2021, 17:27:52) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a83c0f910dbfcbd19468ec888d5b427a34e5a43e434fc22f4c637efaf31b4d30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
